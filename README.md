# Building Perceptron

> "The perceptron is capable of generalization and abstraction; it may recognize similarities between patterns which are not identical." - Frank Rosenblatt

# üìñ Contexte du Projet
Ce projet s'inscrit dans le cadre d'une initiation au Deep Learning √† La Plateforme_. Il vise √† comprendre et impl√©menter le Perceptron, le premier neurone artificiel invent√© par Frank Rosenblatt, consid√©r√© comme la pierre angulaire de l'apprentissage automatique moderne.

L'objectif principal est de d√©velopper une compr√©hension approfondie des concepts fondamentaux du Machine Learning √† travers l'impl√©mentation d'un Perceptron en Python, puis de tester ce mod√®le sur un cas d'usage r√©el avec le dataset Breast Cancer Wisconsin.

# üéØ Objectifs du Projet

## Phase 1 : Fondements Th√©oriques

### Machine Learning vs. Deep Learning : d√©finitions et comparaison

#### Machine Learning (Apprentissage Automatique)

Le Machine Learning regroupe des m√©thodes permettant √† une machine d‚Äôapprendre √† partir de donn√©es sans √™tre explicitement programm√©e pour chaque t√¢che. Les algorithmes classiques incluent :

- R√©gressions lin√©aires et logistiques
- Arbres de d√©cision et for√™ts al√©atoires
- SVM (Support Vector Machines)
- k-nearest neighbors

Ils reposent souvent sur des caract√©ristiques (features) ing√©nieusement extraites et n√©cessitent un pr√©traitement humain des donn√©es (s√©lection, transformation, normalisation).

#### Deep Learning (Apprentissage Profond)

Le Deep Learning est un sous-ensemble du Machine Learning qui utilise des r√©seaux de neurones profonds (plusieurs couches de neurones artificiels) capables d‚Äôapprendre automatiquement les repr√©sentations des donn√©es √† diff√©rents niveaux d‚Äôabstraction (features hi√©rarchiques).

Aspect                            | Machine              | Learning Deep Learning              |
----------------------------------|----------------------|-------------------------------------|
Caract√©ristiques                  | Conception manuelle  | Apprentissage automatique           |
Complexit√© des donn√©es            | Mod√©r√©e              | Tr√®s √©lev√©e (images, sons, textes‚Ä¶) |
Quantit√© de donn√©es               | Moyenne              | Grandes quantit√©s requises          |
Puissance de calcul               | Ordinateur classique | GPU/TPU souvent n√©cessaires         |
Performances sur t√¢ches complexes | Limit√©es             | Excellentes                         |

### Quand utiliser l‚Äôun plut√¥t que l‚Äôautre ?

#### Machine Learning

- Jeux de donn√©es petits √† moyens
- Probl√®mes bien compris o√π les features manuelles sont efficaces
- Contraintes de puissance de calcul

#### Deep Learning

- Donn√©es volumineuses et non structur√©es (images, audio, texte)
- Besoin d‚Äôextraire des repr√©sentations complexes sans expertise m√©tier pouss√©e
- Disponibilit√© de ressources GPU et tol√©rance √† des temps d‚Äôentra√Ænement plus longs

### Applications du Deep Learning

Voici trois exemples embl√©matiques, inspir√©s d‚ÄôAI Experiments et d‚ÄôOpenAI :

#### 1. Diagnostic m√©dical assist√© par imagerie

Des r√©seaux de neurones convolutionnels (CNN) analysent des radiographies ou IRM pour d√©tecter automatiquement des tumeurs ou anomalies. Cette approche acc√©l√®re le diagnostic du cancer et am√©liore la pr√©cision des d√©pistages.

#### 2. V√©hicules autonomes

Les syst√®mes embarqu√©s s‚Äôappuient sur des CNN et des vision transformers pour reconna√Ætre en temps r√©el les pi√©tons, panneaux de signalisation et autres v√©hicules, permettant √† la voiture de naviguer en toute s√©curit√© sans intervention humaine.

#### 3. Traitement automatique du langage naturel (NLP)

Les mod√®les de type Transformers (GPT, BERT) g√®rent la traduction, la g√©n√©ration de texte et les chatbots. Par exemple, ChatGPT comprend et produit un langage proche du dialogue humain pour l‚Äôassistance en ligne ou la cr√©ation de contenu.

### Le Perceptron

##### Introduction

Le Perceptron est le premier mod√®le de neurone artificiel, mis au point par Frank Rosenblatt en 1957. Il sert de brique de base √† l‚Äôapprentissage profond. Ce dossier explique pas √† pas le fonctionnement du perceptron, ses limites, puis montre comment l‚Äô√©tendre et l‚Äôutiliser dans des contextes plus complexes.

#### 1. **Qu‚Äôest-ce qu‚Äôun Perceptron ?**

Un perceptron est un programme informatique qui imite grossi√®rement le comportement d‚Äôun neurone du cerveau :

- **Neurone biologique** : capte des signaux via des extensions appel√©es dendrites, les additionne dans le corps cellulaire, et, si le total d√©passe un seuil, g√©n√®re un signal √©lectrique (potentiel d‚Äôaction) transmis √† d‚Äôautres neurones.

- **Perceptron** : re√ßoit des entr√©es num√©riques x1,x2,‚Ä¶,xn, multiplie chacune par un poids w1,w2,‚Ä¶,wn, ajoute un biais b, puis d√©cide d‚Äôenvoyer un ‚Äúoui‚Äù (1) ou un ‚Äúnon‚Äù (0) selon le r√©sultat.

Cette structure lui permet de distinguer deux cat√©gories d‚Äôobjets ou de donn√©es quand elles sont s√©parables par une ligne droite (en deux dimensions) ou un plan (en plusieurs dimensions).

#### 2. **Formule math√©matique et signification des termes**

La d√©cision du perceptron se calcule ainsi :

<div align="center">
   <img src="images/formule_perceptron.png" alt="y_hat = activation(w1 * x1 + w2 * x2 + ... + wn * xn + b)" style="width: 40%; max-width: 900px;">
</div>

- z : somme pond√©r√©e des entr√©es plus le biais (√©quivalent du potentiel d‚Äôaction).
- f : fonction d‚Äôactivation qui convertit z en 0 ou 1.
- wi : poids attribu√© √† cette entr√©e (force du lien synaptique).
- xi : valeur de la i-√®me entr√©e (ex. intensit√© d‚Äôun pixel).
- b : biais, permet de d√©caler la fronti√®re de d√©cision.

La fonction d‚Äôactivation la plus simple est le seuil de Heaviside :

<div align="center">
   <img src="images/regle_activation.png" alt="Fonction d'activation" style="width: 30%; max-width: 900px;">
</div>

#### 3. Comment apprend-on ?

L‚Äôapprentissage consiste √† ajuster les poids wi et le biais b pour que le perceptron donne la bonne r√©ponse sur des exemples connus. on utlise la r√®gle d'apprentissage du perceptron (mise √† jour des poids) :

<div align="center">
   <img src="images/regle_apprentissage.png" alt="Fonction d'apprentissage" style="width: 30%; max-width: 900px;">
</div>

- Œ∑ : taux d'apprentissage
- y : label attendu
- ≈∑  : pr√©diction du perceptron

Ce formalisme est la base du perceptron simple, utilis√© pour la classification binaire lin√©aire.

1. Calculer la sortie : ≈∑ = f(‚àëwi √ó xi + b)
2. Mesurer l'erreur : Œ¥ = y - ≈∑  
3. Mettre √† jour les poids : wi = wi + Œ∑ √ó Œ¥ √ó xi
4. Mettre √† jour le biais : b = b + Œ∑ √ó Œ¥
    o√π Œ∑ est le taux d‚Äôapprentissage, un petit nombre (ex. 0,01) qui d√©termine la vitesse d‚Äôajustement.

On r√©p√®te ces √©tapes sur tous les exemples, plusieurs fois (plusieurs √©poques), jusqu‚Äô√† obtenir un taux de bonnes r√©ponses satisfaisant.

#### 4. Processus d‚Äôentra√Ænement complet

1. Initialisation al√©atoire : attribuer de petits poids et biais proches de z√©ro.
2. Boucle d‚Äô√©poques :
      - Pour chaque exemple :
            - Pr√©dire y^
            - Calculer l‚Äôerreur Œ¥
            - Ajuster wi et b

3. V√©rification : arr√™ter si l‚Äôerreur moyenne tombe en-dessous d‚Äôun seuil ou apr√®s un nombre fixe d‚Äô√©poques.

#### 5. Limites du Perceptron

- Donn√©es lin√©airement s√©parables : il ne r√©sout que les probl√®mes o√π une ligne ou un plan peut s√©parer les deux classes (ex. XOR n‚Äôest pas s√©parable).
- Convergence non garantie si les donn√©es ne satisfont pas cette condition.
- Fronti√®re plane : incapable de mod√©liser des formes de s√©paration complexes.
- Sensibilit√© aux choix du taux d‚Äôapprentissage et √† l‚Äôinitialisation des poids.

#### 6. Aller plus loin : perceptrons multicouches et backpropagation

Pour traiter des donn√©es non lin√©aires, on assemble plusieurs perceptrons en couches :
- Chaque couche applique la m√™me op√©ration (somme pond√©r√©e + fonction d‚Äôactivation) sur la sortie de la couche pr√©c√©dente.
- On appelle cela un r√©seau de neurones multicouches (MLP).
- L‚Äôalgorithme de r√©tropropagation (backpropagation) calcule les d√©riv√©es de l‚Äôerreur par rapport √† chaque poids en propageant l‚Äôerreur √† rebours, puis ajuste tous les poids simultan√©ment.

#### 7. Versions am√©lior√©es et bonnes pratiques

- Fonctions d‚Äôactivation modernes : ReLU, tanh, sigmo√Øde, qui g√®rent mieux le ph√©nom√®ne de gradient.
- Optimiseurs avanc√©s : Adam, RMSProp, qui adaptent automatiquement le taux d‚Äôapprentissage.
- R√©gularisation : Dropout, weight decay, pour √©viter le surapprentissage.
- Normalisation : Batch Normalization, pour stabiliser et acc√©l√©rer l‚Äôentra√Ænement.

#### 8. Exemples de code Python (orient√© objet)

*python*
import numpy as np

class Perceptron:
    def __init__(self, n_inputs, lr=0.01):
        self.w = np.random.randn(n_inputs) * 0.01
        self.b = 0.0
        self.lr = lr

    def activation(self, z):
        return 1 if z >= 0 else 0

    def predict(self, X):
        z = np.dot(X, self.w) + self.b
        return self.activation(z)

    def train(self, X_train, y_train, epochs=100):
        for _ in range(epochs):
            for x, y in zip(X_train, y_train):
                y_pred = self.predict(x)
                delta = y - y_pred
                self.w += self.lr * delta * x
                self.b += self.lr * delta

Test rapide :
python
# Donn√©es factices
X = np.random.randn(200, 2)
y = np.where(X[:,0] + X[:,1] > 0, 1, 0)

model = Perceptron(n_inputs=2, lr=0.01)
model.train(X, y, epochs=50)

preds = np.array([model.predict(x) for x in X])
print("Pr√©cision :", np.mean(preds == y))

<div style="page-break-after: always;"></div>

## Phase 2 : Impl√©mentation

- D√©velopper une classe Perceptron en Python (programmation orient√©e objet)

- Tester le mod√®le sur des donn√©es factices g√©n√©r√©es al√©atoirement

- Valider le fonctionnement de l'algorithme d'apprentissage

## Phase 3 : Application Pratique

- Analyser le dataset Breast Cancer Wisconsin

- R√©aliser une analyse exploratoire compl√®te des donn√©es

- Appliquer des techniques de r√©duction de dimensionnalit√©

- √âvaluer les performances du Perceptron avec des m√©triques appropri√©es

- Proposer des am√©liorations pour optimiser les r√©sultats

# üìä Donn√©es Utilis√©es

**Dataset** : Breast Cancer Wisconsin
**Source** : UCI Machine Learning Repository

- Probl√©matique : Classification binaire pour le diagnostic du cancer du sein

- Caract√©ristiques : 30 features num√©riques d√©crivant les caract√©ristiques des noyaux cellulaires

- Objectif : Pr√©dire si une tumeur est b√©nigne ou maligne

## Approche d'Analyse

- Nettoyage des donn√©es : Gestion des valeurs manquantes et des outliers

- Analyse exploratoire : Visualisations et statistiques descriptives

- R√©duction de dimensionnalit√© : Application de techniques appropri√©es

- Mod√©lisation : Entra√Ænement du Perceptron d√©velopp√©

- √âvaluation : M√©triques de performance adapt√©es au contexte m√©dical

# üõ†Ô∏è Outils et Technologies

## Langages et Frameworks

- Python : Langage principal du projet

- NumPy : Calculs num√©riques et alg√®bre lin√©aire

- Pandas : Manipulation et analyse de donn√©es

- Matplotlib/Seaborn : Visualisation de donn√©es

- Scikit-learn : Preprocessing et m√©triques d'√©valuation

# Techniques Appliqu√©es

## Programmation Orient√©e Objet : Architecture modulaire du Perceptron

- Analyse Exploratoire : Compr√©hension approfondie des donn√©es

- R√©duction de Dimensionnalit√© : Optimisation des features

- Validation Crois√©e : √âvaluation robuste du mod√®le

# üìÅ Structure du Repository

```
text
building-perceptron/
‚îú‚îÄ‚îÄ .env                    # environnement python
‚îú‚îÄ‚îÄ script.py                 # Classe Perceptron impl√©ment√©e
‚îú‚îÄ‚îÄ notebook.ipynb           # Analyse compl√®te et mod√©lisation
‚îú‚îÄ‚îÄ README.md               # Documentation du projet
‚îú‚îÄ‚îÄ Data/                   # Donn√©es du projet
‚îú‚îÄ‚îÄ visualizations/         # Graphiques et visualisations
‚îî‚îÄ‚îÄ requirements.txt        # D√©pendances Python
```

# üìà R√©sultats et Conclusions

## Performance du Perceptron

[Ins√©rer ici les m√©triques obtenues lors de l'√©valuation]

## Limites Identifi√©es

Le Perceptron pr√©sente certaines limitations inh√©rentes :

- Capacit√© limit√©e aux probl√®mes lin√©airement s√©parables

- Sensibilit√© √† l'initialisation des poids

- Convergence non garantie pour certains datasets

## Am√©liorations Propos√©es

- Impl√©mentation du Perceptron multi-couches

- Utilisation de fonctions d'activation non-lin√©aires

- Application de techniques de r√©gularisation

- Optimisation des hyperparam√®tres

# üìö Bibliographie
## Ressources Acad√©miques

- Rosenblatt, F. (1958). "The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain"

- Minsky, M., & Papert, S. (1969). "Perceptrons: An Introduction to Computational Geometry"

## Ressources Techniques

- Perceptron Algorithm with Code Example ML for beginners!

- Gradient Descent Simply Explained! ML for beginners with Code Example!

- Boruta-py Documentation

## Datasets
- Breast Cancer Wisconsin (Diagnostic) Dataset - UCI Machine Learning Repository

# üîÑ D√©veloppement et Maintenance

Ce projet a √©t√© d√©velopp√© dans le cadre de la formation en Intelligence Artificielle et Data Science √† La Plateforme. Il constitue une base solide pour l'apprentissage des concepts fondamentaux du Machine Learning et peut √™tre √©tendu avec des algorithmes plus complexes.